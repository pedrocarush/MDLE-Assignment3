{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 20:37:16 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/05/31 20:37:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/31 20:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = (SparkSession\n",
    "    .builder\n",
    "    .appName('StructuredStreaming')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 20:37:18 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "items = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', 'localhost')\n",
    "    .option('port', PORT)\n",
    "    .load()\n",
    "    # Treat as CSV\n",
    "    .select(F.split('value', ',', limit=2).alias('split_cols'))\n",
    "    .select(F.to_timestamp(F.col('split_cols')[0]).alias('timestamp'), F.col('split_cols')[1].alias('item'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert items.isStreaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000    # maximum number of bits to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from itertools import groupby\n",
    "\n",
    "class DGIMBuckets(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value):\n",
    "        return []\n",
    "\n",
    "    def addInPlace(self, buckets, new_bit_timestamp):\n",
    "        buckets.append((1, new_bit_timestamp))\n",
    "        buckets_tmp = []\n",
    "        \n",
    "        # keep track of the last merged bucket\n",
    "        merged_bucket = None\n",
    "\n",
    "        # deal first with the smaller buckets, among which the later ones come first\n",
    "        buckets.sort(key=lambda t: (t[0], -t[1]))\n",
    "        for bucket_size, buckets_of_same_size in groupby(buckets, key=lambda t: t[0]):\n",
    "            # sort the buckets themselves by the end timestamp (probably not needed)\n",
    "            buckets_of_same_size = sorted(buckets_of_same_size, key=lambda t: t[1])\n",
    "            print(bucket_size, buckets_of_same_size, merged_bucket)\n",
    "\n",
    "            # if we merged a bucket of the previous size, add it to this batch since it now belongs to it\n",
    "            if merged_bucket is not None:\n",
    "                buckets_of_same_size.append(merged_bucket)\n",
    "\n",
    "            # if more than 2 buckets, which should be no more than 3\n",
    "            if len(buckets_of_same_size) > 2:\n",
    "                # merge the earliest buckets\n",
    "                (bitsum1, _), (bitsum2, end_timestamp2) = buckets_of_same_size[:-1]\n",
    "                merged_bucket = (bitsum1 + bitsum2, end_timestamp2)\n",
    "                print('merged:', merged_bucket)\n",
    "                buckets_tmp.append(buckets_of_same_size[-1])\n",
    "            else:\n",
    "                merged_bucket = None\n",
    "                buckets_tmp.extend(buckets_of_same_size)\n",
    "\n",
    "        if merged_bucket is not None:\n",
    "            buckets_tmp.append(merged_bucket)\n",
    "\n",
    "        buckets.clear()\n",
    "        buckets.extend(buckets_tmp)\n",
    "        buckets.sort(key=lambda t: t[1])\n",
    "\n",
    "        return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgim_buckets = spark.sparkContext.accumulator([], DGIMBuckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgim_process(row: Row):\n",
    "    timestamp = row['timestamp']\n",
    "    bit = row['item']\n",
    "    if bit == '1':\n",
    "        print('Updated with row', row)\n",
    "        dgim_buckets += timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 20:44:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4db62f12-2c4f-475b-9bd2-8e48df614045. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/05/31 20:44:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 12) / 12]\r"
     ]
    }
   ],
   "source": [
    "outputter = (items\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('memory')\n",
    "    .trigger(processingTime='1 second')\n",
    "    .foreach(dgim_process)\n",
    "    # .queryName('outputterMem')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = spark.sql('SELECT * FROM outputterMem')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to quit the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputter.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = (Window\n",
    "     .partitionBy(F.dayofyear(F.col('timestamp')))  # TODO: wrong, choose better partitioning (if at all)\n",
    "     .orderBy(F.desc(F.col('timestamp')))           # descending order so that assignment of timestamp and cummulative sum start at the most recent entries\n",
    "     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "(odf\n",
    "    .withWatermark('timestamp', '5 minutes')\n",
    "    .withColumn('n', F.row_number().over(w) % N)\n",
    "    .withColumn('bitsum', F.sum('item').over(w))\n",
    "    .filter(F.col('n') < N)\n",
    "    .withColumn('bucket', F.floor(F.log2(F.col('bitsum'))))\n",
    "    .sort('timestamp')\n",
    "    .tail(30)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
