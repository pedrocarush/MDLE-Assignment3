{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window, Row, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:26:53 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/06/04 16:26:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/04 16:26:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = (SparkSession\n",
    "    .builder\n",
    "    .appName('StructuredStreaming')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 18:42:56 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "items = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', 'localhost')\n",
    "    .option('port', PORT)\n",
    "    .load()\n",
    "    # Treat as CSV\n",
    "    .select(F.split('value', ',', limit=2).alias('split_cols'))\n",
    "    .select(F.col('split_cols')[0].alias('item'), F.to_timestamp(F.col('split_cols')[1]).alias('timestamp'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert items.isStreaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "bucket_schema = (T.StructType()\n",
    "    .add('bucket_size', T.IntegerType(), nullable=False)\n",
    "    .add('end_timestamp', T.IntegerType(), nullable=False)\n",
    ")\n",
    "# The output Pandas Dataframe has columns bucket_size and end_timestamp\n",
    "output_schema = bucket_schema\n",
    "# The user-defined state is a tuple, containing an array where each element is another tuple containing the bucket_size and end_timestamp\n",
    "state_schema = (T.StructType()\n",
    "    .add('buckets', T.ArrayType(bucket_schema, containsNull=False), nullable=False)\n",
    ")\n",
    "\n",
    "def dgim_update(key: tuple, pdfs: Iterator[pd.DataFrame], state: GroupState, N: int) -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    # Initialize the state and get the old buckets state\n",
    "    if not state.exists:\n",
    "        state.update(([],))\n",
    "    \n",
    "    (buckets,) = state.get\n",
    "\n",
    "    # Merge all Pandas Dataframes into a single Dataframe and sort it so we can know the absolute ordering of the new values\n",
    "    pdf_aggregate = pd.concat(pdfs, ignore_index=True)\n",
    "    pdf_aggregate.sort_values(by=['timestamp'], ascending=False, inplace=True)\n",
    "    pdf_aggregate.reset_index(inplace=True)   # the index is scrambled after sorting, reset it to normal for use as the timestamps in the next loop\n",
    "    # Take the real-time timestamp and replace with integer timestamp\n",
    "    for end_timestamp, bit in pdf_aggregate['item'].items():\n",
    "        if bit == '1':\n",
    "            buckets.append((1, end_timestamp))\n",
    "\n",
    "    maximum_bit_timestamp = pdf_aggregate.shape[0]\n",
    "\n",
    "    # Update the timestamp of the old buckets (they are now older than the oldest in the current batch of bits)\n",
    "    # Additionally, remove the buckets that are too old\n",
    "    buckets = [(bucket_size, end_timestamp + maximum_bit_timestamp) for bucket_size, end_timestamp in buckets if end_timestamp + maximum_bit_timestamp <= N]\n",
    "\n",
    "    new_buckets_merged = []\n",
    "    # Keep track of the last merged bucket\n",
    "    merged_buckets = []\n",
    "\n",
    "    # Deal first with the smaller buckets\n",
    "    buckets.sort(key=lambda t: t[0])\n",
    "    for _, buckets_of_same_size in groupby(buckets, key=lambda t: t[0]):\n",
    "        # Sort the buckets themselves by the end timestamp (earliest first, which are those with larger timestamp)\n",
    "        buckets_of_same_size = sorted(buckets_of_same_size, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "        # If we merged buckets of the previous size, add them to this batch since they now belong to it\n",
    "        if len(merged_buckets) > 0:\n",
    "            buckets_of_same_size.extend(merged_buckets)\n",
    "            merged_buckets.clear()\n",
    "\n",
    "        while len(buckets_of_same_size) > 2:\n",
    "            # Merge the earliest buckets\n",
    "            (bitsum1, _) = buckets_of_same_size.pop(0)\n",
    "            (bitsum2, end_timestamp2) = buckets_of_same_size.pop(0)\n",
    "            merged_buckets.append((bitsum1 + bitsum2, end_timestamp2))\n",
    "\n",
    "        new_buckets_merged.extend(buckets_of_same_size)\n",
    "\n",
    "    # Leftover merged buckets\n",
    "    if len(merged_buckets) > 0:\n",
    "        new_buckets_merged.extend(merged_buckets)\n",
    "\n",
    "    new_buckets_merged.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    state.update((new_buckets_merged,))\n",
    "\n",
    "    yield pd.DataFrame(new_buckets_merged, columns=['bucket_size', 'end_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 19:04:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-523e054e-3ca1-452d-9866-9f1313c69b6a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/04 19:04:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 367:============>                                        (46 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "N = 1000    # N determines whether we should halve the counts for the last bucket (if end_timestamp with bucket_size surpasses N)\n",
    "t = 10      # t determines the frequency with which the counts are computed\n",
    "k = 900     # k determines the window size\n",
    "\n",
    "Ns = f'{N} seconds'\n",
    "ks = f'{k} seconds'\n",
    "ts = f'{t} seconds'\n",
    "\n",
    "outputter = (items\n",
    "    # Disregard items that are too old\n",
    "    .withWatermark('timestamp', '1 hour') \n",
    "            \n",
    "    # Don't group\n",
    "    .withColumn('dummy_key', F.lit(1))\n",
    "    .groupby('dummy_key')\n",
    "    \n",
    "    # Keep and update DGIM buckets over time\n",
    "    .applyInPandasWithState(\n",
    "        lambda key, pdfs, state: dgim_update(key, pdfs, state, N),\n",
    "        outputStructType=output_schema,\n",
    "        stateStructType=state_schema,\n",
    "        outputMode='append',\n",
    "        timeoutConf=GroupStateTimeout.NoTimeout\n",
    "    )\n",
    "\n",
    "    # Add a column with the processing time, and then only consider the latest rows according to it\n",
    "    # We have to do this because applyInPandasWithState does not support 'complete' mode\n",
    "    .withColumn('processing_timestamp', F.current_timestamp())\n",
    "    .groupby('processing_timestamp')\n",
    "    .agg(F.collect_list(F.struct('bucket_size', 'end_timestamp')).alias('computed_buckets'))\n",
    "\n",
    "    # Offer the buckets to be queried\n",
    "    .writeStream\n",
    "    .trigger(processingTime=ts)\n",
    "    .outputMode('complete')\n",
    "    .format('memory')\n",
    "    .queryName('outputterMem')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(bucket_size=32, end_timestamp=97),\n",
       " Row(bucket_size=16, end_timestamp=63),\n",
       " Row(bucket_size=8, end_timestamp=51),\n",
       " Row(bucket_size=8, end_timestamp=37),\n",
       " Row(bucket_size=4, end_timestamp=29),\n",
       " Row(bucket_size=4, end_timestamp=21),\n",
       " Row(bucket_size=2, end_timestamp=15),\n",
       " Row(bucket_size=2, end_timestamp=11),\n",
       " Row(bucket_size=1, end_timestamp=10),\n",
       " Row(bucket_size=1, end_timestamp=9)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 455:=========>                                           (36 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "(spark.sql('SELECT * FROM outputterMem')\n",
    "        .sort('processing_timestamp')\n",
    "        .withColumn('dummy_key', F.lit(1))\n",
    "        .groupby('dummy_key')\n",
    "        .agg(F.explode(F.last('computed_buckets')).alias('computed_buckets_exploded'))\n",
    "        .select(F.col('computed_buckets_exploded').bucket_size.alias('bucket_size'), F.col('computed_buckets_exploded').end_timestamp.alias('end_timestamp'))\n",
    "        .filter(F.col('end_timestamp') <= k)\n",
    ").tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to quit the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 19:08:11 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4a943ca6] is aborting.\n",
      "23/06/04 19:08:11 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4a943ca6] aborted.\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441293,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441294,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441292,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441301,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441303,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441300,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441306,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441302,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441307,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN Shell: Interrupted while joining on: Thread[Thread-441305,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 50.0 in stage 455.0 (TID 43880) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 48.0 in stage 455.0 (TID 43878) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 51.0 in stage 455.0 (TID 43881) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 49.0 in stage 455.0 (TID 43879) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 58.0 in stage 455.0 (TID 43888) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 52.0 in stage 455.0 (TID 43882) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 54.0 in stage 455.0 (TID 43884) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 59.0 in stage 455.0 (TID 43889) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 57.0 in stage 455.0 (TID 43887) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 55.0 in stage 455.0 (TID 43885) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 56.0 in stage 455.0 (TID 43886) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 19:08:11 WARN TaskSetManager: Lost task 53.0 in stage 455.0 (TID 43883) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "outputter.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
