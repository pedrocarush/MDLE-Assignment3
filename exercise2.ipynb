{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window, Row, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 09:54:25 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/06/06 09:54:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/06 09:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = (SparkSession\n",
    "    .builder\n",
    "    .appName('StructuredStreaming')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 09:54:30 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "items = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', 'localhost')\n",
    "    .option('port', PORT)\n",
    "    .load()\n",
    "    # Treat as CSV\n",
    "    .select(F.split('value', ',', limit=2).alias('split_cols'))\n",
    "    .select(F.col('split_cols')[0].alias('item'), F.to_timestamp(F.col('split_cols')[1]).alias('timestamp'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert items.isStreaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "bucket_schema = (T.StructType()\n",
    "    .add('bucket_size', T.IntegerType(), nullable=False)\n",
    "    .add('end_timestamp', T.IntegerType(), nullable=False)\n",
    ")\n",
    "# The output Pandas Dataframe has columns bucket_size and end_timestamp\n",
    "output_schema = bucket_schema\n",
    "# The user-defined state is a tuple, containing an array where each element is another tuple containing the bucket_size and end_timestamp\n",
    "state_schema = (T.StructType()\n",
    "    .add('buckets', T.ArrayType(bucket_schema, containsNull=False), nullable=False)\n",
    ")\n",
    "\n",
    "def dgim_update(key: tuple, pdfs: Iterator[pd.DataFrame], state: GroupState, N: int) -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    # Initialize the state and get the old buckets state\n",
    "    if not state.exists:\n",
    "        state.update(([],))\n",
    "    \n",
    "    (buckets,) = state.get\n",
    "\n",
    "    # Merge all Pandas Dataframes into a single Dataframe and sort it so we can know the absolute ordering of the new values\n",
    "    pdf_aggregate = pd.concat(pdfs, ignore_index=True)\n",
    "    pdf_aggregate.sort_values(by=['timestamp'], ascending=False, inplace=True)\n",
    "    pdf_aggregate.reset_index(inplace=True)   # the index is scrambled after sorting, reset it to normal for use as the timestamps in the next loop\n",
    "    # Take the real-time timestamp and replace with integer timestamp\n",
    "    for end_timestamp, bit in pdf_aggregate['item'].items():\n",
    "        if bit == '1':\n",
    "            buckets.append((1, end_timestamp))\n",
    "\n",
    "    maximum_bit_timestamp = pdf_aggregate.shape[0]\n",
    "\n",
    "    # Update the timestamp of the old buckets (they are now older than the oldest in the current batch of bits)\n",
    "    # Additionally, remove the buckets that are too old\n",
    "    buckets = [(bucket_size, end_timestamp + maximum_bit_timestamp) for bucket_size, end_timestamp in buckets if end_timestamp + maximum_bit_timestamp <= N]\n",
    "\n",
    "    new_buckets_merged = []\n",
    "    # Keep track of the last merged bucket\n",
    "    merged_buckets = []\n",
    "\n",
    "    # Deal first with the smaller buckets\n",
    "    buckets.sort(key=lambda t: t[0])\n",
    "    for _, buckets_of_same_size in groupby(buckets, key=lambda t: t[0]):\n",
    "        # Sort the buckets themselves by the end timestamp (earliest first, which are those with larger timestamp)\n",
    "        buckets_of_same_size = sorted(buckets_of_same_size, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "        # If we merged buckets of the previous size, add them to this batch since they now belong to it\n",
    "        if len(merged_buckets) > 0:\n",
    "            buckets_of_same_size.extend(merged_buckets)\n",
    "            merged_buckets.clear()\n",
    "\n",
    "        while len(buckets_of_same_size) > 2:\n",
    "            # Merge the earliest buckets\n",
    "            (bitsum1, _) = buckets_of_same_size.pop(0)\n",
    "            (bitsum2, end_timestamp2) = buckets_of_same_size.pop(0)\n",
    "            merged_buckets.append((bitsum1 + bitsum2, end_timestamp2))\n",
    "\n",
    "        new_buckets_merged.extend(buckets_of_same_size)\n",
    "\n",
    "    # Leftover merged buckets\n",
    "    if len(merged_buckets) > 0:\n",
    "        new_buckets_merged.extend(merged_buckets)\n",
    "\n",
    "    new_buckets_merged.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    state.update((new_buckets_merged,))\n",
    "\n",
    "    yield pd.DataFrame(new_buckets_merged, columns=['bucket_size', 'end_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 09:54:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0b600c42-961f-4049-ac8b-2b42a88bd562. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/06 09:54:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================>                                (81 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "N = 1000    # N determines whether we should halve the counts for the last bucket (if end_timestamp with bucket_size surpasses N)\n",
    "t = 10      # t determines the frequency with which the counts are computed\n",
    "k = 900     # k determines the window size\n",
    "\n",
    "Ns = f'{N} seconds'\n",
    "ks = f'{k} seconds'\n",
    "ts = f'{t} seconds'\n",
    "\n",
    "outputter = (items\n",
    "    # Disregard items that are too old\n",
    "    .withWatermark('timestamp', '1 hour') \n",
    "            \n",
    "    # Don't group\n",
    "    .withColumn('dummy_key', F.lit(1))\n",
    "    .groupby('dummy_key')\n",
    "    \n",
    "    # Keep and update DGIM buckets over time\n",
    "    .applyInPandasWithState(\n",
    "        lambda key, pdfs, state: dgim_update(key, pdfs, state, N),\n",
    "        outputStructType=output_schema,\n",
    "        stateStructType=state_schema,\n",
    "        outputMode='append',\n",
    "        timeoutConf=GroupStateTimeout.NoTimeout\n",
    "    )\n",
    "\n",
    "    # Add a column with the processing time, and then only consider the latest rows according to it\n",
    "    # We have to do this because applyInPandasWithState does not support 'complete' mode\n",
    "    .withColumn('processing_timestamp', F.current_timestamp())\n",
    "    .groupby('processing_timestamp')\n",
    "    .agg(F.collect_list(F.struct('bucket_size', 'end_timestamp')).alias('computed_buckets'))\n",
    "\n",
    "    # Offer the buckets to be queried\n",
    "    .writeStream\n",
    "    .trigger(processingTime=ts)\n",
    "    .outputMode('complete')\n",
    "    .format('memory')\n",
    "    .queryName('outputterMem')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(total_ones=59)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:========================>                             (89 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "col_dgim_error_correction = F.when(F.first('end_timestamp') + F.first('bucket_size') > k, F.floor(F.first('bucket_size') / 2)).otherwise(0)\n",
    "\n",
    "(spark.sql('SELECT * FROM outputterMem')\n",
    "        .sort('processing_timestamp')\n",
    "        .withColumn('dummy_key', F.lit(1))\n",
    "        .groupby('dummy_key')\n",
    "        .agg(F.explode(F.last('computed_buckets')).alias('computed_buckets_exploded'))\n",
    "        .select(F.col('computed_buckets_exploded').bucket_size.alias('bucket_size'), F.col('computed_buckets_exploded').end_timestamp.alias('end_timestamp'))\n",
    "        .filter(F.col('end_timestamp') <= k)\n",
    "        .agg((F.sum('bucket_size') - col_dgim_error_correction).alias('total_ones'))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 489:=====================================>              (146 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "metrics = (spark.sql('SELECT * FROM outputterMem')\n",
    "        .select('processing_timestamp', F.explode('computed_buckets').alias('computed_buckets_exploded'))\n",
    "        .select('processing_timestamp', F.col('computed_buckets_exploded').bucket_size.alias('bucket_size'), F.col('computed_buckets_exploded').end_timestamp.alias('end_timestamp'))\n",
    "        .groupby('processing_timestamp')\n",
    "        .agg((F.sum('bucket_size') - col_dgim_error_correction).alias('total_ones'))\n",
    ").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to quit the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 10:20:02 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@684fddcd] is aborting.\n",
      "23/06/06 10:20:02 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 150, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@684fddcd] aborted.\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650726,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650713,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650714,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650722,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650721,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650728,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650720,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650723,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650724,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650738,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN Shell: Interrupted while joining on: Thread[Thread-650729,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:65)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:132)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:172)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 56.0 in stage 494.0 (TID 62013) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 57.0 in stage 494.0 (TID 62014) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 66.0 in stage 494.0 (TID 62023) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 55.0 in stage 494.0 (TID 62012) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 61.0 in stage 494.0 (TID 62018) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 60.0 in stage 494.0 (TID 62017) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 62.0 in stage 494.0 (TID 62019) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 58.0 in stage 494.0 (TID 62015) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 64.0 in stage 494.0 (TID 62021) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 65.0 in stage 494.0 (TID 62022) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:02 WARN TaskSetManager: Lost task 63.0 in stage 494.0 (TID 62020) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/06 10:20:03 WARN FileUtil: Failed to delete file or dir [/tmp/temporary-0b600c42-961f-4049-ac8b-2b42a88bd562/state/0/107]: it still exists.\n",
      "23/06/06 10:20:03 WARN HDFSBackedStateStoreProvider: Error cleaning up files for HDFSStateStoreProvider[id = (op=0,part=107),dir = file:/tmp/temporary-0b600c42-961f-4049-ac8b-2b42a88bd562/state/0/107]\n",
      "java.lang.IllegalArgumentException: requirement failed\n",
      "\tat scala.Predef$.require(Predef.scala:268)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.filesForVersion(HDFSBackedStateStoreProvider.scala:671)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.cleanup(HDFSBackedStateStoreProvider.scala:650)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.doMaintenance(HDFSBackedStateStoreProvider.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2(StateStore.scala:604)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2$adapted(StateStore.scala:602)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:586)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:446)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/06 10:20:03 WARN FileUtil: Failed to delete file or dir [/tmp/temporary-0b600c42-961f-4049-ac8b-2b42a88bd562/state/0/193]: it still exists.\n",
      "23/06/06 10:20:03 WARN HDFSBackedStateStoreProvider: Error cleaning up files for HDFSStateStoreProvider[id = (op=0,part=193),dir = file:/tmp/temporary-0b600c42-961f-4049-ac8b-2b42a88bd562/state/0/193]\n",
      "java.lang.IllegalArgumentException: requirement failed\n",
      "\tat scala.Predef$.require(Predef.scala:268)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.filesForVersion(HDFSBackedStateStoreProvider.scala:671)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.cleanup(HDFSBackedStateStoreProvider.scala:650)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.doMaintenance(HDFSBackedStateStoreProvider.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2(StateStore.scala:604)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2$adapted(StateStore.scala:602)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:586)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:446)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "outputter.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 489:===============================================>    (183 + 12) / 200]\r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 494:===========>                                         (44 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timestamps = [metric['processing_timestamp'] for metric in metrics]\n",
    "total_ones_over_time = [metric['total_ones'] for metric in metrics]\n",
    "plt.scatter(timestamps, total_ones_over_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.imshow( mpimg.imread('results/dgim_actual_ones.png') )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Decaying Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "counts_schema = (T.StructType()\n",
    "    .add('item', T.StringType())\n",
    "    .add('count', T.DoubleType())\n",
    ")\n",
    "# The output Pandas Dataframe has columns item and count\n",
    "output_schema = counts_schema\n",
    "# The user-defined state is a tuple, containing a map of item counts\n",
    "state_schema = (T.StructType()\n",
    "    .add('counts', T.MapType(T.StringType(), T.DoubleType(), valueContainsNull=False), nullable=False)\n",
    ")\n",
    "\n",
    "def exponential_count_update(key: tuple, pdfs: Iterator[pd.DataFrame], state: GroupState, c: float, score_threshold: float) -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    # Initialize the state and get the old counts state\n",
    "    if not state.exists:\n",
    "        state.update(({},))\n",
    "    \n",
    "    (counts,) = state.get\n",
    "\n",
    "    # Merge all Pandas Dataframes into a single Dataframe and sort it so we can know the absolute ordering of the new values\n",
    "    # Also reset the index so that we can get proper ordering\n",
    "    pdf_aggregate = pd.concat(pdfs, ignore_index=True)\n",
    "    pdf_aggregate.sort_values(by=['timestamp'], ascending=False, inplace=True)\n",
    "    \n",
    "    time_elaped = pdf_aggregate.shape[0]\n",
    "    pdf_aggregate['time_step'] = range(time_elaped)\n",
    "\n",
    "    # Update old counts (make them older)\n",
    "    old_decay = ((1 - c)**time_elaped)\n",
    "    counts = {item: count * old_decay for item, count in counts.items() if count * old_decay >= score_threshold}\n",
    "\n",
    "    # Add new counts\n",
    "    for item, group in pdf_aggregate.groupby(by='item'):\n",
    "        counts.setdefault(item, 0)\n",
    "        counts[item] += float(((1 - c)**group['time_step']).sum())\n",
    "\n",
    "    state.update((counts,))\n",
    "\n",
    "    counts_items = counts.items()\n",
    "    yield pd.DataFrame({'item': [i for i, _ in counts_items], 'count': [c for _, c in counts_items]}, columns=['item', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:08:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f6a41a05-0371-4b2f-9301-12567be079f3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/04 21:08:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:08:57 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 5442 milliseconds\n"
     ]
    }
   ],
   "source": [
    "c = 1e-6\n",
    "score_threshold = 1 / 2\n",
    "\n",
    "outputter = (items\n",
    "    # Disregard items that are too old\n",
    "    .withWatermark('timestamp', '1 hour') \n",
    "            \n",
    "    # Don't group\n",
    "    .withColumn('dummy_key', F.lit(1))\n",
    "    .groupby('dummy_key')\n",
    "    \n",
    "    # Keep and update exponentially decaying counts over time\n",
    "    .applyInPandasWithState(\n",
    "        lambda key, pdfs, state: exponential_count_update(key, pdfs, state, c, score_threshold),\n",
    "        outputStructType=output_schema,\n",
    "        stateStructType=state_schema,\n",
    "        outputMode='append',\n",
    "        timeoutConf=GroupStateTimeout.NoTimeout\n",
    "    )\n",
    "\n",
    "    # Add a column with the processing time, and then only consider the latest rows according to it\n",
    "    # We have to do this because applyInPandasWithState does not support 'complete' mode\n",
    "    .withColumn('processing_timestamp', F.current_timestamp())\n",
    "    .groupby('processing_timestamp')\n",
    "    .agg(F.collect_list(F.struct('item', 'count')).alias('computed_counts'))\n",
    "\n",
    "    # Offer the counts to be queried\n",
    "    .writeStream\n",
    "    .trigger(processingTime='1 seconds')\n",
    "    .outputMode('complete')\n",
    "    .format('memory')\n",
    "    .queryName('outputterMem')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(item='E', count=2.9999180011299877),\n",
       " Row(item='B', count=4.999911001086988),\n",
       " Row(item='D', count=6.999883001490982),\n",
       " Row(item='C', count=7.999805002761966),\n",
       " Row(item='A', count=16.999664004645943)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:09:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 4685 milliseconds\n",
      "[Stage 214:===============>                                     (60 + 12) / 200]\r"
     ]
    }
   ],
   "source": [
    "(spark.sql('SELECT * FROM outputterMem')\n",
    "        .sort('processing_timestamp')\n",
    "        .withColumn('dummy_key', F.lit(1))\n",
    "        .groupby('dummy_key')\n",
    "        .agg(F.explode(F.last('computed_counts')).alias('computed_counts_exploded'))\n",
    "        .select(F.col('computed_counts_exploded').item.alias('item'), F.col('computed_counts_exploded').count.alias('count'))\n",
    "        .sort('count', ascending=True)\n",
    ").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:09:55 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@28fc9484] is aborting.\n",
      "23/06/04 21:09:55 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@28fc9484] aborted.\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121247,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121248,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 ERROR TaskContextImpl: Error in TaskCompletionListener) / 200]\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-f6a41a05-0371-4b2f-9301-12567be079f3/state/1/77 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:65)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:132)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:172)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 WARN TaskSetManager: Lost task 77.0 in stage 214.0 (TID 12423) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121251,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121249,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121253,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:56 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-f6a41a05-0371-4b2f-9301-12567be079f3/state/1/76 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:65)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:132)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:172)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 76.0 in stage 214.0 (TID 12422) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN Shell: Interrupted while joining on: Thread[Thread-121261,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:56 WARN Shell: Interrupted while joining on: Thread[Thread-121258,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:55 WARN Shell: Interrupted while joining on: Thread[Thread-121250,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:56 WARN Shell: Interrupted while joining on: Thread[Thread-121257,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$4(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasWithStateExec.timeTakenMs(FlatMapGroupsInPandasWithStateExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExecBase.$anonfun$processDataWithPartition$3(FlatMapGroupsWithStateExec.scala:178)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 80.0 in stage 214.0 (TID 12426) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 73.0 in stage 214.0 (TID 12419) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 82.0 in stage 214.0 (TID 12428) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 72.0 in stage 214.0 (TID 12418) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 79.0 in stage 214.0 (TID 12425) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 83.0 in stage 214.0 (TID 12429) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 75.0 in stage 214.0 (TID 12421) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 74.0 in stage 214.0 (TID 12420) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 81.0 in stage 214.0 (TID 12427) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n",
      "23/06/04 21:09:56 WARN TaskSetManager: Lost task 78.0 in stage 214.0 (TID 12424) (192.168.1.67 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "outputter.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
