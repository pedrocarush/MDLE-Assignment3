{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window, Row, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/01 11:38:15 WARN Utils: Your hostname, martinho-SATELLITE-L50-B resolves to a loopback address: 127.0.1.1; using 192.168.57.24 instead (on interface wlx200db038271f)\n",
      "23/06/01 11:38:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/01 11:38:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = (SparkSession\n",
    "    .builder\n",
    "    .appName('StructuredStreaming')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/01 11:38:28 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "items = (spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', 'localhost')\n",
    "    .option('port', PORT)\n",
    "    .load()\n",
    "    # Treat as CSV\n",
    "    # .select(F.split('value', ',', limit=2).alias('split_cols'))\n",
    "    # .select(F.to_timestamp(F.col('split_cols')[0]).alias('timestamp'), F.col('split_cols')[1].alias('item'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert items.isStreaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000    # maximum number of bits to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from itertools import groupby\n",
    "\n",
    "class DGIMBuckets(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value):\n",
    "        return []\n",
    "\n",
    "    def addInPlace(self, buckets, new_bit_timestamp):\n",
    "        buckets.append((1, new_bit_timestamp))\n",
    "        buckets_tmp = []\n",
    "        \n",
    "        # keep track of the last merged bucket\n",
    "        merged_bucket = None\n",
    "\n",
    "        # deal first with the smaller buckets, among which the later ones come first\n",
    "        buckets.sort(key=lambda t: (t[0], -t[1]))\n",
    "        for bucket_size, buckets_of_same_size in groupby(buckets, key=lambda t: t[0]):\n",
    "            # sort the buckets themselves by the end timestamp (probably not needed)\n",
    "            buckets_of_same_size = sorted(buckets_of_same_size, key=lambda t: t[1])\n",
    "            print(bucket_size, buckets_of_same_size, merged_bucket)\n",
    "\n",
    "            # if we merged a bucket of the previous size, add it to this batch since it now belongs to it\n",
    "            if merged_bucket is not None:\n",
    "                buckets_of_same_size.append(merged_bucket)\n",
    "\n",
    "            # if more than 2 buckets, which should be no more than 3\n",
    "            if len(buckets_of_same_size) > 2:\n",
    "                # merge the earliest buckets\n",
    "                (bitsum1, _), (bitsum2, end_timestamp2) = buckets_of_same_size[:-1]\n",
    "                merged_bucket = (bitsum1 + bitsum2, end_timestamp2)\n",
    "                print('merged:', merged_bucket)\n",
    "                buckets_tmp.append(buckets_of_same_size[-1])\n",
    "            else:\n",
    "                merged_bucket = None\n",
    "                buckets_tmp.extend(buckets_of_same_size)\n",
    "\n",
    "        if merged_bucket is not None:\n",
    "            buckets_tmp.append(merged_bucket)\n",
    "\n",
    "        buckets.clear()\n",
    "        buckets.extend(buckets_tmp)\n",
    "        buckets.sort(key=lambda t: t[1])\n",
    "\n",
    "        return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgim_buckets = spark.sparkContext.accumulator([], DGIMBuckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgim_process(row: Row):\n",
    "    timestamp = row['timestamp']\n",
    "    bit = row['item']\n",
    "    if bit == '1':\n",
    "        print('Updated with row', row)\n",
    "        dgim_buckets += timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGIMProcessor:\n",
    "\n",
    "    def __init__(self, dgim_buckets: DGIMBuckets):\n",
    "        self.dgim_buckets = dgim_buckets\n",
    "\n",
    "    def process(self, row: Row):\n",
    "        timestamp = row['timestamp']\n",
    "        bit = row['item']\n",
    "        if bit == '1':\n",
    "            print('Updated with row', row)\n",
    "            self.dgim_buckets += timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'withColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ts \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m w \u001b[39m=\u001b[39m (Window\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#  .partitionBy(F.dayofyear(F.col('timestamp')))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m      \u001b[39m.\u001b[39morderBy(F\u001b[39m.\u001b[39mdesc(F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m)))           \u001b[39m# descending order so that assignment of timestamp and cummulative sum start at the most recent entries\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m      \u001b[39m.\u001b[39mrowsBetween(\u001b[39m-\u001b[39mk, Window\u001b[39m.\u001b[39mcurrentRow)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputter \u001b[39m=\u001b[39m (items\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mwithColumn(\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m, F\u001b[39m.\u001b[39;49mcurrent_timestamp())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39;49mgroupby(F\u001b[39m.\u001b[39;49mwindow(\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1 day\u001b[39;49m\u001b[39m'\u001b[39;49m, ts))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m.\u001b[39;49mwithColumn(\u001b[39m'\u001b[39m\u001b[39mrow_number\u001b[39m\u001b[39m'\u001b[39m, F\u001b[39m.\u001b[39mrow_number()\u001b[39m.\u001b[39mover(w))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m.\u001b[39mwithWatermark(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m, Ns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m.\u001b[39mwriteStream\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m.\u001b[39mtrigger(processingTime\u001b[39m=\u001b[39mts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m.\u001b[39moutputMode(\u001b[39m'\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m.\u001b[39mqueryName(\u001b[39m'\u001b[39m\u001b[39moutputterMem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m.\u001b[39mstart()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/martinho/ua/4-2/mdle/assign3/exercise2.ipynb#X64sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'withColumn'"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "t = 5\n",
    "k = 900\n",
    "\n",
    "Ns = f'{N} seconds'\n",
    "ts = f'{t} seconds'\n",
    "\n",
    "w = (Window\n",
    "    #  .partitionBy(F.dayofyear(F.col('timestamp')))\n",
    "     .orderBy(F.desc(F.col('timestamp')))           # descending order so that assignment of timestamp and cummulative sum start at the most recent entries\n",
    "     .rowsBetween(-k, Window.currentRow)\n",
    ")\n",
    "\n",
    "outputter = (items\n",
    "    .withColumn('timestamp', F.current_timestamp())\n",
    "    .groupby(F.window('timestamp', '1 day', ts))\n",
    "    .\n",
    "    .withColumn('row_number', F.row_number().over(w))\n",
    "    .withWatermark('timestamp', Ns)\n",
    "    \n",
    "    .writeStream\n",
    "    .trigger(processingTime=ts)\n",
    "    .outputMode('append')\n",
    "    .format('memory')\n",
    "    .queryName('outputterMem')\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 50, 1000)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 50, 1000)),\n",
       " Row(value='1', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 50, 1000)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 50, 1000)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 55)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 55)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 55)),\n",
       " Row(value='1', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 55)),\n",
       " Row(value='0', timestamp=datetime.datetime(2023, 6, 1, 11, 55, 55))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM outputterMem').tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to quit the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputter.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = (Window\n",
    "     .partitionBy(F.dayofyear(F.col('timestamp')))  # TODO: wrong, choose better partitioning (if at all)\n",
    "     .orderBy(F.desc(F.col('timestamp')))           # descending order so that assignment of timestamp and cummulative sum start at the most recent entries\n",
    "     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "(odf\n",
    "    .withWatermark('timestamp', '5 minutes')\n",
    "    .withColumn('n', F.row_number().over(w) % N)\n",
    "    .withColumn('bitsum', F.sum('item').over(w))\n",
    "    .filter(F.col('n') < N)\n",
    "    .withColumn('bucket', F.floor(F.log2(F.col('bitsum'))))\n",
    "    .sort('timestamp')\n",
    "    .tail(30)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
